{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e49a2f",
   "metadata": {},
   "source": [
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743e45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAPI_API_KEY\"]= \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79e2776",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3540849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q youtube-transcript-api langchain-community langchain-openai faiss-cpu tiktoken python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d5b27de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi,TranscriptsDisabled\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b59499e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transcript fetched\n",
      "<class 'list'> 117\n",
      "[{'text': 'تخيّل أنّك وجدت نص', 'start': 1.14, 'duration': 2.836}, {'text': 'يصف مشهداً بين شخص ومساعده يصف حديثًا بين شخص و مساعد وهمي يعمل بواسطة الذكاء الاصطناعي.', 'start': 3.976, 'duration': 2.464}, {'text': 'يعمل بالذكاء الاصطناعي.', 'start': 6.44, 'duration': 0.7}]\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
    "\n",
    "video_id = \"LPZh9BOjkQs\"\n",
    "\n",
    "try:\n",
    "    # this uses the ‘fetch’ method as recommended in docs\n",
    "    fetched = YouTubeTranscriptApi().fetch(video_id, languages=[\"ar\"])\n",
    "    transcript_list = fetched.to_raw_data()  # if available, otherwise convert manually\n",
    "    print(\"✅ Transcript fetched\")\n",
    "    print(type(transcript_list), len(transcript_list))\n",
    "    print(transcript_list[:3])\n",
    "\n",
    "except TranscriptsDisabled:\n",
    "    print(\"Transcripts are disabled for this video.\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbb202db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Imagine you happen across a short movie script that', 'start': 1.14, 'duration': 2.836}, {'text': 'describes a scene between a person and their AI assistant.', 'start': 3.976, 'duration': 3.164}, {'text': \"The script has what the person asks the AI, but the AI's response has been torn off.\", 'start': 7.48, 'duration': 5.58}, {'text': 'Suppose you also have this powerful magical machine that can take', 'start': 13.06, 'duration': 3.92}, {'text': 'any text and provide a sensible prediction of what word comes next.', 'start': 16.98, 'duration': 3.98}]\n"
     ]
    }
   ],
   "source": [
    "print(transcript_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afe0cc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript string created ✅\n",
      "7524 characters\n",
      "Imagine you happen across a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose you also have this powerful magical machine that can take any text and provide a sensible predic\n"
     ]
    }
   ],
   "source": [
    "transcript = \" \".join(chunk[\"text\"] for chunk in transcript_list)\n",
    "print(\"Transcript string created ✅\")\n",
    "print(len(transcript), \"characters\")\n",
    "print(transcript[:300])  # preview first part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0f2b770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 10\n",
      "Imagine you happen across a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose you also have this powerful magical machine that can take any text and provide a sensible predic\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "chunks = splitter.create_documents([transcript])\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "print(chunks[0].page_content[:300])  # preview first chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be6d2976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0560f786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"Imagine you happen across a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next. You could then finish the script by feeding in what you have to the machine, seeing what it would predict to start the AI's answer, and then repeating this over and over with a growing script completing the dialogue. When you interact with a chatbot, this is exactly what's happening. A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text. Instead of predicting one word with certainty, though, what it does is assign a probability to all possible next words. To build a chatbot, you lay out some text that describes an interaction between a user and a hypothetical AI assistant, add on whatever the\")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26f8659a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-openai) (1.0.1)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-openai) (2.6.1)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.4.38)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (2.12.3)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-openai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.10.23)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai) (0.11.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai) (2.12.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai\n",
    "!pip install openai --upgrade\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19f279d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "927da1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-1.0.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (0.4)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain) (1.0.1)\n",
      "Collecting langgraph<1.1.0,>=1.0.0 (from langchain)\n",
      "  Using cached langgraph-1.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (0.4.38)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Using cached langgraph_checkpoint-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.0 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Using cached langgraph_prebuilt-1.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Using cached langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Using cached xxhash-3.6.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Using cached ormsgpack-1.11.0-cp313-cp313-win_amd64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-openai) (2.6.1)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.10.23)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (2.0.44)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (3.13.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (2.3.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n",
      "Downloading langchain-1.0.2-py3-none-any.whl (107 kB)\n",
      "Using cached langgraph-1.0.1-py3-none-any.whl (155 kB)\n",
      "Using cached langgraph_checkpoint-3.0.0-py3-none-any.whl (46 kB)\n",
      "Using cached langgraph_prebuilt-1.0.1-py3-none-any.whl (28 kB)\n",
      "Using cached langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "Using cached ormsgpack-1.11.0-cp313-cp313-win_amd64.whl (112 kB)\n",
      "Using cached xxhash-3.6.0-cp313-cp313-win_amd64.whl (31 kB)\n",
      "Installing collected packages: xxhash, ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain\n",
      "\n",
      "   ---------------------------------------- 0/7 [xxhash]\n",
      "   ----- ---------------------------------- 1/7 [ormsgpack]\n",
      "   ----- ---------------------------------- 1/7 [ormsgpack]\n",
      "   ----------- ---------------------------- 2/7 [langgraph-sdk]\n",
      "   ----------- ---------------------------- 2/7 [langgraph-sdk]\n",
      "   ----------- ---------------------------- 2/7 [langgraph-sdk]\n",
      "   ----------- ---------------------------- 2/7 [langgraph-sdk]\n",
      "   ----------------- ---------------------- 3/7 [langgraph-checkpoint]\n",
      "   ----------------- ---------------------- 3/7 [langgraph-checkpoint]\n",
      "   ----------------- ---------------------- 3/7 [langgraph-checkpoint]\n",
      "   ----------------- ---------------------- 3/7 [langgraph-checkpoint]\n",
      "   ----------------- ---------------------- 3/7 [langgraph-checkpoint]\n",
      "   ---------------------- ----------------- 4/7 [langgraph-prebuilt]\n",
      "   ---------------------- ----------------- 4/7 [langgraph-prebuilt]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------- ----------- 5/7 [langgraph]\n",
      "   ---------------------------------- ----- 6/7 [langchain]\n",
      "   ---------------------------------- ----- 6/7 [langchain]\n",
      "   ---------------------------------- ----- 6/7 [langchain]\n",
      "   ---------------------------------- ----- 6/7 [langchain]\n",
      "   ---------------------------------- ----- 6/7 [langchain]\n",
      "   ---------------------------------- ----- 6/7 [langchain]\n",
      "   ---------------------------------- ----- 6/7 [langchain]\n",
      "   ---------------------------------- ----- 6/7 [langchain]\n",
      "   ---------------------------------- ----- 6/7 [langchain]\n",
      "   ---------------------------------------- 7/7 [langchain]\n",
      "\n",
      "Successfully installed langchain-1.0.2 langgraph-1.0.1 langgraph-checkpoint-3.0.0 langgraph-prebuilt-1.0.1 langgraph-sdk-0.2.9 ormsgpack-1.11.0 xxhash-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain langchain-openai langchain-community faiss-cpu\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9b3317cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fb475ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ.pop(\"OPENAI_API_KEY\", None)\n",
    "os.environ.pop(\"HUGGINGFACEHUB_API_KEY\", None)\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-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\"\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bcbe3700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAPI_API_KEY = hf_aQwCWgD...\n",
      "OPENAI_API_KEY = sk-proj-hf...\n"
     ]
    }
   ],
   "source": [
    "for k, v in os.environ.items():\n",
    "    if \"KEY\" in k.upper():\n",
    "        print(k, \"=\", v[:10] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e8560927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated Key: sk-proj-hf...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Remove the wrong key\n",
    "os.environ.pop(\"OPENAPI_API_KEY\", None)\n",
    "\n",
    "# Force the correct key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-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\"\n",
    "\n",
    "print(\"✅ Updated Key:\", os.getenv(\"OPENAI_API_KEY\")[:10] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f522c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Remove the wrong key\n",
    "os.environ.pop(\"OPENAPI_API_KEY\", None)\n",
    "\n",
    "# Force the correct key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75151568",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m chunks = text_splitter.split_documents(docs)\n\u001b[32m     12\u001b[39m embeddings = OpenAIEmbeddings(model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-3-small\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m vector_store = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Vector store created successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ritik\\OneDrive\\Desktop\\Gen Ai\\myenv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:805\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    803\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ritik\\OneDrive\\Desktop\\Gen Ai\\myenv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ritik\\OneDrive\\Desktop\\Gen Ai\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:625\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    623\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    624\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ritik\\OneDrive\\Desktop\\Gen Ai\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:514\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    518\u001b[39m         response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ritik\\OneDrive\\Desktop\\Gen Ai\\myenv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ritik\\OneDrive\\Desktop\\Gen Ai\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ritik\\OneDrive\\Desktop\\Gen Ai\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "print(\"✅ Vector store created locally without OpenAI API!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b5111c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1640243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.9.0-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Collecting filelock (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.8/12.0 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.2/12.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.4/12.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.0 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 5.8 MB/s  0:00:02\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 2.9 MB/s  0:00:00\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Downloading torch-2.9.0-cp313-cp313-win_amd64.whl (109.3 MB)\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.6/109.3 MB 12.5 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 5.2/109.3 MB 12.2 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 7.6/109.3 MB 12.0 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 9.2/109.3 MB 11.7 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 11.3 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 11.1 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 16.5/109.3 MB 10.9 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 18.9/109.3 MB 10.9 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 20.7/109.3 MB 10.7 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 23.1/109.3 MB 10.7 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 25.4/109.3 MB 10.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 27.3/109.3 MB 10.6 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 10.5 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 10.5 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 33.8/109.3 MB 10.5 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 35.9/109.3 MB 10.5 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 38.0/109.3 MB 10.5 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 40.1/109.3 MB 10.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 42.2/109.3 MB 10.5 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 44.3/109.3 MB 10.4 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 46.7/109.3 MB 10.4 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 48.8/109.3 MB 10.4 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 51.1/109.3 MB 10.4 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 53.2/109.3 MB 10.4 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 55.3/109.3 MB 10.4 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 57.7/109.3 MB 10.4 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 59.8/109.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 61.9/109.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 64.2/109.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 66.3/109.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 68.4/109.3 MB 10.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 70.8/109.3 MB 10.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 72.9/109.3 MB 10.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 75.0/109.3 MB 10.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 77.1/109.3 MB 10.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 79.2/109.3 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 81.3/109.3 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 83.4/109.3 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 85.7/109.3 MB 10.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 87.8/109.3 MB 10.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 89.9/109.3 MB 10.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 91.8/109.3 MB 10.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 94.1/109.3 MB 10.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 96.2/109.3 MB 10.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 98.3/109.3 MB 10.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 100.4/109.3 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 102.5/109.3 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 104.6/109.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  107.0/109.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 109.3/109.3 MB 8.6 MB/s  0:00:12\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.3-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "   ---------------------------------------- 0.0/38.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.4/38.5 MB 11.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 4.7/38.5 MB 11.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 7.3/38.5 MB 11.8 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 9.4/38.5 MB 11.4 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 11.5/38.5 MB 11.2 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 13.6/38.5 MB 10.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 15.2/38.5 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 17.8/38.5 MB 10.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.9/38.5 MB 10.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.3/38.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.4/38.5 MB 10.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 26.7/38.5 MB 10.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 28.8/38.5 MB 10.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.2/38.5 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.3/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.4/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.5/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.5/38.5 MB 8.0 MB/s  0:00:04\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, scipy, safetensors, networkx, joblib, fsspec, filelock, torch, scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   -- -------------------------------------  1/15 [threadpoolctl]\n",
      "   -- -------------------------------------  1/15 [threadpoolctl]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   ---------- -----------------------------  4/15 [safetensors]\n",
      "   ---------- -----------------------------  4/15 [safetensors]\n",
      "   ------------- --------------------------  5/15 [networkx]\n",
      "   ------------- --------------------------  5/15 [networkx]\n",
      "   ------------- --------------------------  5/15 [networkx]\n",
      "   ------------- --------------------------  5/15 [networkx]\n",
      "   ------------- --------------------------  5/15 [networkx]\n",
      "   ------------- --------------------------  5/15 [networkx]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'c:\\\\Users\\\\ritik\\\\OneDrive\\\\Desktop\\\\Gen Ai\\\\myenv\\\\Lib\\\\site-packages\\\\networkx\\\\generators\\\\tests\\\\test_sudoku.py'\n",
      "Check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6e62b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (2.9.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa0dc581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store created locally without OpenAI API!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "print(\"✅ Vector store created locally without OpenAI API!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0d05930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from faiss-cpu) (2.3.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from faiss-cpu) (24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f6a0b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (2.9.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f503143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Local, free embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdc9f2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store created successfully!\n"
     ]
    }
   ],
   "source": [
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "print(\"✅ Vector store created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "849349c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'fc2d9b4b-e19f-4bf6-bdca-03ea2067a3f2',\n",
       " 1: '57ae7917-db79-4244-bd14-3066d324d4c9',\n",
       " 2: '8b2bb7ce-f5ba-40ec-af57-4bfad9791407',\n",
       " 3: 'c49be6e8-386c-4251-b01f-d402168f05bb',\n",
       " 4: 'f46bdce5-2ed9-4c66-9d79-6a69ebf3131f',\n",
       " 5: '0716bd68-2409-4f68-8a2e-c7f82a935ec9',\n",
       " 6: 'dfd6d4d4-ab14-42cf-ae6c-c54b1f0e1fd5',\n",
       " 7: 'd6e35e61-7ed4-4865-80b2-17fee3374313',\n",
       " 8: '62f3ee8e-7c63-4a3b-bfeb-317fa2ed6958',\n",
       " 9: '9528cfb5-5169-44f6-a5e4-3283815f9247',\n",
       " 10: 'cd592fd1-9d96-4a2d-972a-10840c95ebbe',\n",
       " 11: 'ee770cdf-1ee5-422e-9ae5-e5268b5a5557',\n",
       " 12: 'f597fc04-3a67-4d91-98e2-a2eddcf94af2',\n",
       " 13: '75fc4493-c7e6-434f-b8a2-826026fe3536',\n",
       " 14: 'bc0fb452-4ed5-4c3d-8f6c-bfda69ae4e01',\n",
       " 15: 'ee46ad0f-c5a9-4ae0-99a7-9f6746b5faf0',\n",
       " 16: 'ad694f2a-42b7-42e7-848d-0a8a107553ae',\n",
       " 17: '4aa96fce-e1e9-4399-9336-c442e53972f0',\n",
       " 18: 'a30e775d-a5ba-46b4-b5a7-7b2a352fcd4c',\n",
       " 19: '15e7d1f3-a8e8-422c-bd99-d576cc5cfd61',\n",
       " 20: '781f3b1d-9928-49ff-9b35-95db59e91b12',\n",
       " 21: 'a482b4d8-a89f-4c40-9fdf-e847d6e3ebb4',\n",
       " 22: 'ea8fc1eb-3992-4cd8-ab72-7ab791387351',\n",
       " 23: 'e293e522-8689-44d3-a5eb-c04ee2e3acf6',\n",
       " 24: '39511bfc-6ff2-4c4e-9056-f6eef20b93c1',\n",
       " 25: 'a9433df3-f26a-469d-8388-a2009f047957',\n",
       " 26: 'fa2e7e6d-4949-4bd7-8319-d244451e50ee'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "452d7094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a9433df3-f26a-469d-8388-a2009f047957', metadata={'producer': 'Corel PDF Engine Version 16.0.0.707', 'creator': 'CorelDRAW X6', 'creationdate': '2020-05-14T10:21:27+05:30', 'moddate': '2020-05-14T10:21:27+05:30', 'author': 'Administrator', 'title': 'Untitled-1', 'source': 'example.pdf', 'total_pages': 28, 'page': 25, 'page_label': '26'}, page_content='26\\nNursery Rhymes and Songs\\n_amR>r - {d^mJ\\n1. XmoZ hmoVr A§S>r\\nË`m§Zm dmObr W§S>r\\nXmV bmJbo dmOm`bm\\nA§S>r bmJbr ZmMm`bm\\nZmMVm ZmMVm Q>¸$a Pmbr\\nXmoÝhr A§S>r \\\\w$Qy>Z Jobr.\\n2. Mma ggo {nQw>H$bo, Ë`m§Zr ~g{dbo ZmQw>H$bo\\nZmQ>H$mMo Zmd H$m`, Imbr S>moH§$ da nm`\\nEH$ ggm Kgabm, Xþgam ZmQ>H$ {dgabm\\n{Vgam Jobm PmonyZ, Mm¡Wm ~gbm bnyZ\\nT>_T>_ dmObr S>\\\\$, AmVm J§_V ~K\\nYy_ nimbo ggo, AmVm ZmQ>H$ hmoUma H$go ?')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.get_by_ids(['a9433df3-f26a-469d-8388-a2009f047957'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f665cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-6.1.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Downloading pypdf-6.1.3-py3-none-any.whl (323 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-6.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c23dcff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pypdf installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pypdf\n",
    "print(\"pypdf installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c2e6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-openai) (1.0.1)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-openai) (2.6.1)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.4.38)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (2.12.3)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-openai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.10.23)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (2.0.44)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (3.13.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-community) (2.3.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai langchain-community faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "999dc16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7765b9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000024BB73A5BD0>, search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f79728d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ad694f2a-42b7-42e7-848d-0a8a107553ae', metadata={'producer': 'Corel PDF Engine Version 16.0.0.707', 'creator': 'CorelDRAW X6', 'creationdate': '2020-05-14T10:21:27+05:30', 'moddate': '2020-05-14T10:21:27+05:30', 'author': 'Administrator', 'title': 'Untitled-1', 'source': 'example.pdf', 'total_pages': 28, 'page': 16, 'page_label': '17'}, page_content='17\\nNursery Rhymes and Songs\\n22.  Ring a ring of roses\\nRing a ring of roses\\nPocket full of posies\\nA Tishoo and A Tishoo\\nWe all fall down\\nThe cows are in the meadows\\nEating buttercups\\nA Tishoo and A Tishoo\\nWe all jump up'),\n",
       " Document(id='fc2d9b4b-e19f-4bf6-bdca-03ea2067a3f2', metadata={'producer': 'Corel PDF Engine Version 16.0.0.707', 'creator': 'CorelDRAW X6', 'creationdate': '2020-05-14T10:21:27+05:30', 'moddate': '2020-05-14T10:21:27+05:30', 'author': 'Administrator', 'title': 'Untitled-1', 'source': 'example.pdf', 'total_pages': 28, 'page': 0, 'page_label': '1'}, page_content=\"Shikshana Prasaraka Mandali's\\nS.P.M. Public School\\nRHYMESRHYMES\\nANDAND\\nSONGSSONGS\\nRHYMES\\nAND\\nSONGS\\nNursery\"),\n",
       " Document(id='f597fc04-3a67-4d91-98e2-a2eddcf94af2', metadata={'producer': 'Corel PDF Engine Version 16.0.0.707', 'creator': 'CorelDRAW X6', 'creationdate': '2020-05-14T10:21:27+05:30', 'moddate': '2020-05-14T10:21:27+05:30', 'author': 'Administrator', 'title': 'Untitled-1', 'source': 'example.pdf', 'total_pages': 28, 'page': 12, 'page_label': '13'}, page_content='17.  Rainbow\\nThere is rainbow in the sky\\nDo you know how it came by?\\nDrops of water through sunlight\\nGives the sky a pretty sight\\nSeven colours can be seen \\nRed, yellow and lovely green\\nViolet, Indigo and blue \\nThere is orange in it too!\\n13\\nNursery Rhymes and Songs'),\n",
       " Document(id='75fc4493-c7e6-434f-b8a2-826026fe3536', metadata={'producer': 'Corel PDF Engine Version 16.0.0.707', 'creator': 'CorelDRAW X6', 'creationdate': '2020-05-14T10:21:27+05:30', 'moddate': '2020-05-14T10:21:27+05:30', 'author': 'Administrator', 'title': 'Untitled-1', 'source': 'example.pdf', 'total_pages': 28, 'page': 13, 'page_label': '14'}, page_content='14\\nNursery Rhymes and Songs\\nBits of paper\\nBits of paper\\n                              Lying on the floor\\n                              Lying on the floor\\nMakes the place untidy\\nMakes the place untidy\\n                              Pick them up\\n                              Pick them up\\nCollect all the papers\\nCollect all the papers\\n                              Where shall we throw?\\n                              Where shall we throw?\\nThrow them in the dustbin\\nThrow them in the dustbin\\n                              Now the place is clean\\n                              Now the place is clean\\n18.  Bits of paper'),\n",
       " Document(id='ee770cdf-1ee5-422e-9ae5-e5268b5a5557', metadata={'producer': 'Corel PDF Engine Version 16.0.0.707', 'creator': 'CorelDRAW X6', 'creationdate': '2020-05-14T10:21:27+05:30', 'moddate': '2020-05-14T10:21:27+05:30', 'author': 'Administrator', 'title': 'Untitled-1', 'source': 'example.pdf', 'total_pages': 28, 'page': 11, 'page_label': '12'}, page_content=\"12\\nNursery Rhymes and Songs\\n16.  Open them Shut them\\nOpen them, shut them(2)\\nGive a little clap(2)\\nOpen them, shut them(2)\\nLay them on your lap(2)\\nCreep them(4)\\nGo upto your chin(2)\\nOpen wide your little mouth\\nBut don't let them in.\\n15.  AEROPLANE AEROPLANE\\nAeroplane, Aeroplane up in the sky,(2)\\nWhere are you going flying so high\\nOver the hills over the seas,(2)\\nAeroplane Aeroplane please take me\")]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver.invoke('what is poems')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92037601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3466870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate  \n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a helpful assistant.\n",
    "Answer only from the provided transcript context.\n",
    "If the context is insufficient, just say you don't know.\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    input_variables=['context', 'question']  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4e9167b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain) (1.0.1)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain) (1.0.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (0.4.38)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (1.0.1)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain) (1.11.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ritik\\onedrive\\desktop\\gen ai\\myenv\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7cd2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "question =  \"is the topic of poem discussed  in this video?if yes  then  what was discussed\"\n",
    "retrieved_docs = retriver.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a345dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5cfda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = prompt.invoke({\"context\":  context_text, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ebc73b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"You are a helpful assistant.\\nAnswer only from the provided transcript context.\\nIf the context is insufficient, just say you don't know.\\n17.  Rainbow\\nThere is rainbow in the sky\\nDo you know how it came by?\\nDrops of water through sunlight\\nGives the sky a pretty sight\\nSeven colours can be seen \\nRed, yellow and lovely green\\nViolet, Indigo and blue \\nThere is orange in it too!\\n13\\nNursery Rhymes and Songs\\n\\nIndex\\n1.    See me\\n2.    Baa, Baa Black Sheep\\n3.    Heads and shoulders\\n4.    Incy wincy spider\\n5.    Oh dear!\\n6.    I'm a little teapot\\n7.    One, two, buckle my shoe\\n8.    Row Row Row Your Boat\\n9.    Hot cross buns\\n10.  Two little dicky birds\\n11.  I Hear thunder\\n12.  Tooth Brush\\n13.  Ten little fingers\\n14.  Clap your hands\\n15.  Aeroplane Aeroplane\\n1\\nNursery Rhymes and Songs\\n16.  Open them Shut them\\n17.  Rainbow\\n18.  Bits of paper\\n19.  Where is Thumbkin?\\n20.  Two little hands\\n21.  The bear went over the mountain\\n22.  Ring a ring of roses\\n23.  Munching mangoes\\n24. Vegetables\\n25. I have Two Arms...\\n26. Rabbit, Rabbit\\n27. One, Two\\n28.  {hÝX {d^mJ\\n29.  _amR>r {d^mJ\\n\\n17\\nNursery Rhymes and Songs\\n22.  Ring a ring of roses\\nRing a ring of roses\\nPocket full of posies\\nA Tishoo and A Tishoo\\nWe all fall down\\nThe cows are in the meadows\\nEating buttercups\\nA Tishoo and A Tishoo\\nWe all jump up\\n\\n6\\nNursery Rhymes and Songs\\nOh dear! What can the matter be?\\nOh dear! What can the matter be?\\nOh dear! What can the matter be?\\nJohnny's so long at the fair.\\nHe promised to buy me a bunch of blue ribbons,\\nTo tie up my long brown hair.\\nOh dear! What can the matter be?\\nOh dear! What can the matter be?\\nOh dear! What can the matter be?\\nJohnny's so long at the fair.\\n5.  Oh Dear!\\n\\nTwo little hands go\\n                            Clap clap clap\\nTwo little feet go\\n                            Tap tap tap\\nTwo little eyes\\n                            That open wide\\nOne little head\\n                            Nods side to side\\n21. The Bear went over the mountain\\nThe bear went over the mountain (3)\\nTo see what he could see\\nAnd what did you think he saw? (2)\\nThe other side of the mountain (3)\\nWas all that he could see\\nAnd what did you think he did? (2)\\nHe slipped down the mountain (3)\\nAnd fell into the sea\\nSplash………….\\n20.  Two little hands\\n16\\nNursery Rhymes and Songs\\nQuestion: is the topic of poem discussed  in this video?if yes  then  what was discussed\\n\")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "686ddcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the topic of the poem \"Rainbow\" is discussed in the video. It describes how a rainbow appears in the sky, created by drops of water reflecting sunlight, and lists the seven colors that can be seen in a rainbow: red, yellow, green, violet, indigo, blue, and orange.\n"
     ]
    }
   ],
   "source": [
    "answer = llm.invoke(final_prompt)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "558c0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "69f3f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(retrieved_docs):\n",
    "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "  return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5dfd98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel({\n",
    "  'context': retriver | RunnableLambda(format_docs),\n",
    "  'question': RunnablePassthrough()\n",
    "\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "acaf4564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': \"Index\\n1.    See me\\n2.    Baa, Baa Black Sheep\\n3.    Heads and shoulders\\n4.    Incy wincy spider\\n5.    Oh dear!\\n6.    I'm a little teapot\\n7.    One, two, buckle my shoe\\n8.    Row Row Row Your Boat\\n9.    Hot cross buns\\n10.  Two little dicky birds\\n11.  I Hear thunder\\n12.  Tooth Brush\\n13.  Ten little fingers\\n14.  Clap your hands\\n15.  Aeroplane Aeroplane\\n1\\nNursery Rhymes and Songs\\n16.  Open them Shut them\\n17.  Rainbow\\n18.  Bits of paper\\n19.  Where is Thumbkin?\\n20.  Two little hands\\n21.  The bear went over the mountain\\n22.  Ring a ring of roses\\n23.  Munching mangoes\\n24. Vegetables\\n25. I have Two Arms...\\n26. Rabbit, Rabbit\\n27. One, Two\\n28.  {hÝX {d^mJ\\n29.  _amR>r {d^mJ\\n\\n17\\nNursery Rhymes and Songs\\n22.  Ring a ring of roses\\nRing a ring of roses\\nPocket full of posies\\nA Tishoo and A Tishoo\\nWe all fall down\\nThe cows are in the meadows\\nEating buttercups\\nA Tishoo and A Tishoo\\nWe all jump up\\n\\n17.  Rainbow\\nThere is rainbow in the sky\\nDo you know how it came by?\\nDrops of water through sunlight\\nGives the sky a pretty sight\\nSeven colours can be seen \\nRed, yellow and lovely green\\nViolet, Indigo and blue \\nThere is orange in it too!\\n13\\nNursery Rhymes and Songs\\n\\n14\\nNursery Rhymes and Songs\\nBits of paper\\nBits of paper\\n                              Lying on the floor\\n                              Lying on the floor\\nMakes the place untidy\\nMakes the place untidy\\n                              Pick them up\\n                              Pick them up\\nCollect all the papers\\nCollect all the papers\\n                              Where shall we throw?\\n                              Where shall we throw?\\nThrow them in the dustbin\\nThrow them in the dustbin\\n                              Now the place is clean\\n                              Now the place is clean\\n18.  Bits of paper\\n\\n7\\nNursery Rhymes and Songs\\n6.  I'm a little Teapot\\nI'm a little teapot \\nShort and stout \\nThis is my handle \\nThis is my spout\\nWhen the water's boiling\\nHear me shout\\nPick me up and pour me out\\n7.  One, two, buckle my shoe\\nOne, two, buckle my shoe\\nThree, four, knock at the door\\nFive, six, pick up the sticks,\\nSeven, eight, lay them straight\\nNine, ten, a big fat hen\",\n",
       " 'question': 'Poem'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_chain.invoke('Poem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ce65dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4bf0902",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_chain = parallel_chain | prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b29558bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have a little tooth brush  \\nI hold it very tight  \\nI brush my teeth each morning  \\nAnd then again at night  '"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_chain.invoke('can you tell me the poem Tooth Brush')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f7a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
